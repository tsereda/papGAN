apiVersion: v1
kind: Pod
metadata:
  name: preprocess-pod
spec:
  securityContext:
    runAsUser: 1000
    runAsGroup: 100
    fsGroup: 100
  containers:
  - name: pap-processing
    image: gitlab-registry.nrp-nautilus.io/prp/jupyter-stack/prp
    env:
    - name: REPO_PATH
      value: /opt/repo/papGAN
    command:
    - "bash"
    - "-c"
    args:
    - |
      # --- User and Environment Setup ---
      echo "Running as user: $(whoami), UID: $(id -u), GID: $(id -g)"
      
      # --- Git Repository Update ---
      echo "Cloning Git repository..."
      cd /opt/repo
      git clone --single-branch -b main https://github.com/tsereda/papGAN
      cd ${REPO_PATH}
      echo "Git repository cloned."
      
      sudo apt-get update && sudo apt-get install -y p7zip-full
      
      # --- Download dataset ---
      if [ ! -f /data/isbi2025-ps3c-train-dataset.7z ]; then
        pip install gdown
        echo "Downloading dataset 7z file..."
        gdown 1yeHKqXVf9YhS6FAqVGezKrSaNGp6v-xX -O /data/isbi2025-ps3c-train-dataset.7z
      else
        echo "Dataset 7z file already exists."
      fi
      
      # Use native 7z command for faster extraction with parallel processing
      echo "Copying dataset 7z to /dev/shm/..."
      # -aoa: Overwrite all existing files without prompt# Copy to local storage
      # First copy the 7z file to faster storage (using memory-backed tmpfs)
      cp /data/isbi2025-ps3c-train-dataset.7z /dev/shm/

      # Then extract from the faster location
      echo "Extracting dataset..."
      7z x /dev/shm/isbi2025-ps3c-train-dataset.7z -o/opt/repo/papGAN/isbi2025-ps3c-train-dataset -aoa -mmt=on

      # Clean up the temporary copy when done
      rm /dev/shm/isbi2025-ps3c-train-dataset.7z
      echo "Extraction complete!"
      
      # --- Run preprocessing ---
      echo "Running preprocessing..."
      cd ${REPO_PATH}
  
      
      # Run the preprocessing script with the extracted dataset path
      echo "Running preprocess.py with: --source_dir /opt/repo/papGAN/isbi2025-ps3c-train-dataset"
      python preprocess.py --source_dir /opt/repo/papGAN/isbi2025-ps3c-train-dataset
      
      echo "Archiving processed data with parallel compression..."
      sudo apt-get install -y pigz
      
      # Check output
      echo "Checking output directories:"
      ls -la
      
      tar -cf - cyclegan_dataset_256_split | pigz -9 -p 4 > cyclegan_processed_data.tar.gz
      
      echo "Preprocessing process completed. Data should be at cyclegan_processed_data.tar.gz"
      
      # Add a sleep at the end to keep the pod running
      echo "Processing completed, sleeping to keep pod alive..."
      sleep infinity
    volumeMounts:
    - name: git-repo
      mountPath: /opt/repo
    - name: pap-data-volume
      mountPath: /data
    - name: dshm
      mountPath: /dev/shm
    resources:
      requests:
        memory: 20Gi
        cpu: "10"
      limits:
        memory: 24Gi
        cpu: "12"
  volumes:
  - name: git-repo
    emptyDir: {}
  - name: pap-data-volume
    persistentVolumeClaim:
      claimName: pap-data
  - name: dshm
    emptyDir:
      medium: Memory
      sizeLimit: 20Gi